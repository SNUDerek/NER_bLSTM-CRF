{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "generate integer-indexed sentences, pos-tags and named entity tags, dictionaries for converting, etc, and save as `npy` binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import get_vocab, index_sents\n",
    "from embedding import create_embeddings\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum network vocabulary, test set size\n",
    "MAX_VOCAB = 25000\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read ConLL2002 NER corpus from csv (first save as utf-8!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ner_dataset_utf8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence: 1', 'nan', 'nan', 'nan', 'nan']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentmarks = data[\"Sentence #\"].tolist()\n",
    "sentmarks = [str(s) for s in sentmarks]\n",
    "sentmarks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data[\"Word\"].tolist()\n",
    "postags = data[\"POS\"].tolist()\n",
    "nertags = data[\"Tag\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = []\n",
    "sentence_post = []\n",
    "sentence_ners = []\n",
    "\n",
    "vocab = []\n",
    "\n",
    "this_snt = []\n",
    "this_pos = []\n",
    "this_ner = []\n",
    "\n",
    "for idx, s in enumerate(sentmarks):\n",
    "    # reset if new sent\n",
    "    if s != 'nan':\n",
    "        # edit: ONLY IF HAS TAG!\n",
    "    \n",
    "        if len(this_snt) > 0 and this_snt[-1] == '0':\n",
    "            if list(set(this_ner)) != ['O']:\n",
    "                sentence_text.append(this_snt[:-1])\n",
    "                sentence_post.append(this_pos[:-1])\n",
    "                sentence_ners.append(this_ner[:-1])\n",
    "        this_snt = []\n",
    "        this_pos = []\n",
    "        this_ner = []\n",
    "    \n",
    "    # add to lists \n",
    "    this_snt.append(words[idx].lower())\n",
    "    this_pos.append(postags[idx])\n",
    "    this_ner.append(nertags[idx])\n",
    "    vocab.append(words[idx].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country']\n",
      "['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['they', 'marched', 'from', 'the', 'houses', 'of', 'parliament', 'to', 'a', 'rally', 'in', 'hyde', 'park']\n",
      "['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'NNP', 'NNP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(sentence_text[:2]):\n",
    "    print(sent)\n",
    "    print(sentence_post[idx])\n",
    "    print(sentence_ners[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get vocabulary and index inputs\n",
    "\n",
    "we need to convert the string input to integer vectors for the `keras` network (the `pycrfsuite` network needs strings, as it will extract feature vectors from words themselves).\n",
    "\n",
    "we will index each word from 1 according to inverse frequency (most common word is 1, etc.) until the max-vocab size. We will reserve two slots, 0 for the PAD index, and MAX_VOCAB-1 for out-of-vocabulary or unknown words (OOV/UNK). Since this is boring stuff, I've put it in external functions. Packages like `keras` and `sklearn` have more robust tools for this, but a simple word:index dictionary will do fine for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vocab dicts\n",
    "# subtract 2 for UNK, PAD\n",
    "word2idx, idx2word = get_vocab(sentence_text, MAX_VOCAB-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS and NER tag vocab dicts\n",
    "pos2idx, idx2pos = get_vocab(sentence_post, len(set(postags)))\n",
    "ner2idx, idx2ner = get_vocab(sentence_ners, len(set(nertags))+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "sentence_text_idx = index_sents(sentence_text, word2idx)\n",
    "sentence_post_idx = index_sents(sentence_post, pos2idx)\n",
    "sentence_ners_idx = index_sents(sentence_ners, ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-test splitting\n",
    "\n",
    "we divide the training data into training data, and testing data. the testing data is used only for checking model performance. a third set, the *validation set*, may be split off from our training data for hyperparameter tuning, although if we use k-fold cross-validation, our validation set will change every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(sentence_text))]\n",
    "\n",
    "train_idx, test_idx, X_train_pos, X_test_pos = train_test_split(indices, sentence_post_idx, test_size=TEST_SIZE)\n",
    "\n",
    "def get_sublist(lst, indices):\n",
    "    result = []\n",
    "    for idx in indices:\n",
    "        result.append(lst[idx])\n",
    "    return result\n",
    "\n",
    "X_train_sents = get_sublist(sentence_text_idx, train_idx)\n",
    "X_test_sents = get_sublist(sentence_text_idx, test_idx)\n",
    "y_train_ner = get_sublist(sentence_ners_idx, train_idx)\n",
    "y_test_ner = get_sublist(sentence_ners_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create word2vec embeddings for words, pos-tags\n",
    "\n",
    "using pre-trained embedding vectors to initialize the embedding layer has been shown to help training for various sequence labeling tasks such as POS tagging (Huang, Xu & Yu 2015; Ma & Hovy 2016) and Named Entity Recognition for English (Ma & Hovy 2016; Lee Changki 2017) and Japanese (Misawa, Taniguchi, Miura & Ohkuma 2017).\n",
    "\n",
    "because we are using the POS-tags as a secondary input, we will also train an embedding space fo these. we will use only the training data to create the embeddings. i am using `gensim` for this task, and i am using a helper function to wrap the `Word2Vec` that saves the embedding and also the vocabulary dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "train_sent_texts = [sentence_text[idx] for idx in train_idx]\n",
    "        \n",
    "w2v_vocab, w2v_model = create_embeddings(train_sent_texts,\n",
    "                       embeddings_path='embeddings/text_embeddings.gensimmodel',\n",
    "                       vocab_path='embeddings/text_mapping.json',\n",
    "                       size=300,\n",
    "                       workers=4,\n",
    "                       iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos embeddings\n",
    "train_post_texts = [sentence_post[idx] for idx in train_idx]\n",
    "\n",
    "w2v_pvocab, w2v_pmodel = create_embeddings(train_post_texts,\n",
    "                         embeddings_path='embeddings/pos_embeddings.gensimmodel',\n",
    "                         vocab_path='embeddings/pos_mapping.json',\n",
    "                         size=100,\n",
    "                         workers=4,\n",
    "                         iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save everything to numpy binaries for loading\n",
    "\n",
    "granted, `pickle` would probably be more suitable for a lot of these things. but over-reliance on `numpy` binaries is a bad habit i've picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_save(saves, names):\n",
    "    for idx, item in enumerate(saves):\n",
    "        np.save('encoded/{0}.npy'.format(names[idx]), item)\n",
    "    return\n",
    "\n",
    "saves = [\n",
    "vocab,\n",
    "sentence_text_idx,\n",
    "sentence_post_idx,\n",
    "sentence_ners_idx,\n",
    "word2idx, idx2word,\n",
    "pos2idx, idx2pos,\n",
    "ner2idx, idx2ner,\n",
    "train_idx,\n",
    "test_idx,\n",
    "X_train_sents,\n",
    "X_test_sents,\n",
    "X_train_pos,\n",
    "X_test_pos,\n",
    "y_train_ner,\n",
    "y_test_ner,\n",
    "sentence_text,\n",
    "sentence_post,\n",
    "sentence_ners]\n",
    "\n",
    "names = [\n",
    "'vocab',\n",
    "'sentence_text_idx',\n",
    "'sentence_post_idx',\n",
    "'sentence_ners_idx',\n",
    "'word2idx', 'idx2word',\n",
    "'pos2idx', 'idx2pos',\n",
    "'ner2idx', 'idx2ner',\n",
    "'train_idx',\n",
    "'test_idx',\n",
    "'X_train_sents',\n",
    "'X_test_sents',\n",
    "'X_train_pos',\n",
    "'X_test_pos',\n",
    "'y_train_ner',\n",
    "'y_test_ner',\n",
    "'sentence_text',\n",
    "'sentence_post',\n",
    "'sentence_ners']\n",
    "\n",
    "numpy_save(saves, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
